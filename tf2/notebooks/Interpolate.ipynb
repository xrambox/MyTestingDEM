{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation Example\n",
    "This notebook demonstrates an interpolation example where we interpolate the function given by `exact_sol(x)` at a discrete set of points.\n",
    "\n",
    "---\n",
    "\n",
    "## Imports and Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tfp_loss import tfp_function_factory\n",
    "from utils.Plotting import plot_convergence_semilog\n",
    "\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition\n",
    "Define the neural network model class that includes the forward pass, gradient computation, custom loss function, and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model(tf.keras.Model): \n",
    "    def __init__(self, layers, train_op, num_epoch, print_epoch):\n",
    "        super(model, self).__init__()\n",
    "        self.model_layers = layers\n",
    "        self.train_op = train_op\n",
    "        self.num_epoch = num_epoch\n",
    "        self.print_epoch = print_epoch\n",
    "        self.adam_loss_hist = []\n",
    "    \n",
    "    def call(self, X):\n",
    "        return self.u(X)\n",
    "    \n",
    "    # Running the model\n",
    "    def u(self,X):\n",
    "        for l in self.model_layers:\n",
    "            X = l(X)\n",
    "        return X\n",
    "    \n",
    "    # Return the first derivative\n",
    "    def du(self, X):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(X)\n",
    "            u_val = self.u(X)\n",
    "        du_val = tape.gradient(u_val, X)\n",
    "        return du_val\n",
    "         \n",
    "    #Custom loss function\n",
    "    def get_loss(self,Xint, Yint):\n",
    "        u_val_int=self.u(Xint)\n",
    "        int_loss = tf.reduce_mean(tf.math.square(u_val_int - Yint))\n",
    "        return int_loss\n",
    "      \n",
    "    # get gradients\n",
    "    def get_grad(self, Xint, Yint):\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.trainable_variables)\n",
    "            L = self.get_loss(Xint, Yint)\n",
    "        g = tape.gradient(L, self.trainable_variables)\n",
    "        return L, g\n",
    "      \n",
    "    # perform gradient descent\n",
    "    def network_learn(self,Xint,Yint):\n",
    "        for i in range(self.num_epoch):\n",
    "            L, g = self.get_grad(Xint, Yint)\n",
    "            self.train_op.apply_gradients(zip(g, self.trainable_variables))\n",
    "            self.adam_loss_hist.append(L)\n",
    "            if i%self.print_epoch==0:\n",
    "                print(\"Epoch {} loss: {}\".format(i, L))\n",
    "\n",
    "#define the function ot be interpolated\n",
    "k = 4\n",
    "def exact_sol(input):\n",
    "    output = np.sin(k*np.pi*input)\n",
    "    return output\n",
    "\n",
    "#define the input and output data set\n",
    "xmin = -1\n",
    "xmax = 1\n",
    "numPts = 201\n",
    "data_type = \"float64\"\n",
    "\n",
    "Xint = np.linspace(xmin, xmax, numPts).astype(data_type)\n",
    "Xint = np.array(Xint)[np.newaxis].T\n",
    "Yint = exact_sol(Xint)\n",
    "\n",
    "#define the model \n",
    "tf.keras.backend.set_floatx(data_type)\n",
    "l1 = tf.keras.layers.Dense(64, \"tanh\")\n",
    "l2 = tf.keras.layers.Dense(64, \"tanh\")\n",
    "l3 = tf.keras.layers.Dense(1, None)\n",
    "train_op = tf.keras.optimizers.Adam()\n",
    "num_epoch = 10000\n",
    "print_epoch = 100\n",
    "pred_model = model([l1, l2, l3], train_op, num_epoch, print_epoch)\n",
    "\n",
    "#convert the training data to tensors\n",
    "Xint_tf = tf.convert_to_tensor(Xint)\n",
    "Yint_tf = tf.convert_to_tensor(Yint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training\n",
    "print(\"Training (ADAM)...\")\n",
    "pred_model.network_learn(Xint_tf, Yint_tf)\n",
    "print(\"Training (LBFGS)...\")\n",
    "loss_func = tfp_function_factory(pred_model, Xint_tf, Yint_tf)\n",
    "# convert initial model parameters to a 1D tf.Tensor\n",
    "init_params = tf.dynamic_stitch(loss_func.idx, pred_model.trainable_variables)\n",
    "# train the model with L-BFGS solver\n",
    "results = tfp.optimizer.lbfgs_minimize(\n",
    "    value_and_gradients_function=loss_func, initial_position=init_params,\n",
    "         max_iterations=4000, num_correction_pairs=50, tolerance=1e-14)  \n",
    "# after training, the final optimized parameters are still in results.position\n",
    "# so we have to manually put them back to the model\n",
    "loss_func.assign_new_model_parameters(results.position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing...\")\n",
    "numPtsTest = 2*numPts\n",
    "x_test = np.linspace(xmin, xmax, numPtsTest)    \n",
    "x_test = np.array(x_test)[np.newaxis].T\n",
    "x_tf = tf.convert_to_tensor(x_test)\n",
    "\n",
    "y_test = pred_model.u(x_tf)    \n",
    "y_exact = exact_sol(x_test)\n",
    "\n",
    "plt.plot(x_test, y_test, x_test, y_exact)\n",
    "plt.show()\n",
    "plt.plot(x_test, y_exact-y_test)\n",
    "plt.title(\"Error\")\n",
    "plt.show()\n",
    "err = y_exact - y_test\n",
    "print(\"L2-error norm: {}\".format(np.linalg.norm(err)/np.linalg.norm(y_exact)))\n",
    "\n",
    "# plot the loss convergence\n",
    "plot_convergence_semilog(pred_model.adam_loss_hist, loss_func.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2+2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
